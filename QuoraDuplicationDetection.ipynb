{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QuoraDuplicationDetection.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4k7tCCpZ7uY"
      },
      "source": [
        "\n",
        "# get dataset from Quora in tsv format\n",
        "\n",
        "import requests\n",
        "\n",
        "r = requests.get(\"http://qim.fs.quoracdn.net/quora_duplicate_questions.tsv\")\n",
        "with open(\"QuoraDuplicationDataset.tsv\", \"wb\") as f:\n",
        "    f.write(r.content)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfQABNI7cCuB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "outputId": "40e89452-e10e-4958-df4a-3f91fea42bbc"
      },
      "source": [
        "# read dataset\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "df = pd.read_csv('QuoraDuplicationDataset.tsv', sep = \"\\t\")\n",
        "df.drop(['qid1','qid2'], axis=1, inplace=True)\n",
        "print(df)\n",
        "df.dropna(axis=0, inplace=True)\n",
        "df.groupby(\"is_duplicate\")['id'].count().plot.bar()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "            id  ... is_duplicate\n",
            "0            0  ...            0\n",
            "1            1  ...            0\n",
            "2            2  ...            0\n",
            "3            3  ...            0\n",
            "4            4  ...            0\n",
            "...        ...  ...          ...\n",
            "404285  404285  ...            0\n",
            "404286  404286  ...            1\n",
            "404287  404287  ...            0\n",
            "404288  404288  ...            0\n",
            "404289  404289  ...            0\n",
            "\n",
            "[404290 rows x 4 columns]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7faff1aae358>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiPlua_woj38"
      },
      "source": [
        "# apply preprocessing techniques\n",
        "\n",
        "import re\n",
        "\n",
        "def convert_to_lower_case(text):\n",
        "    \n",
        "    return text.lower()\n",
        "\n",
        "def remove_punctuation_and_white_space(text):\n",
        "\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(' +', ' ', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "def general_removal(text):\n",
        "\n",
        "    text = re.sub(\"\\'s\", \" \", text)\n",
        "    text = re.sub(\" whats \", \" what is \", text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"\\'ve\", \" have \", text)\n",
        "    text = re.sub(\"can't\", \"can not\", text)\n",
        "    text = re.sub(\"n't\", \" not \", text)\n",
        "    text = re.sub(\"i'm\", \"i am\", text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"\\'re\", \" are \", text)\n",
        "    text = re.sub(\"\\'d\", \" would \", text)\n",
        "    text = re.sub(\"\\'ll\", \" will \", text)\n",
        "    text = re.sub(\"e\\.g\\.\", \" eg \", text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"b\\.g\\.\", \" bg \", text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"(\\d+)(kK)\", \" \\g<1>000 \", text)\n",
        "    text = re.sub(\"e-mail\", \" email \", text, flags=re.IGNORECASE)\n",
        "    text = re.sub('(?<=[0-9])\\,(?=[0-9])', \"\", text)\n",
        "    text = re.sub('\\$', \" dollar \", text)\n",
        "    text = re.sub('\\%', \" percent \", text)\n",
        "    text = re.sub('\\&', \" and \", text)\n",
        "\n",
        "    return text\n",
        "\n",
        "def preprocessing(text):\n",
        "    text = convert_to_lower_case(text)\n",
        "    text = remove_punctuation_and_white_space(text)\n",
        "    text = general_removal(text)\n",
        "    return text\n",
        "\n",
        "df['question1'] = df['question1'].apply(preprocessing)\n",
        "df['question2'] = df['question2'].apply(preprocessing)\n",
        "print(df['question1'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zzy0JZhGOVh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "outputId": "0c4b4d50-ba12-4144-8a3a-b2c949fbfc91"
      },
      "source": [
        "# bag of words implementation\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import scipy\n",
        "from sklearn.model_selection import train_test_split\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import f1_score, classification_report, accuracy_score\n",
        "\n",
        "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
        "count_vect.fit(pd.concat((df['question1'],df['question2'])).unique())\n",
        "trainq1_trans = count_vect.transform(df['question1'].values)\n",
        "trainq2_trans = count_vect.transform(df['question2'].values)\n",
        "labels = df['is_duplicate'].values\n",
        "X = scipy.sparse.hstack((trainq1_trans,trainq2_trans))\n",
        "y = labels\n",
        "X_train,X_valid,y_train,y_valid = train_test_split(X,y, test_size = 0.33, random_state = 42)\n",
        "\n",
        "xgb_model = xgb.XGBClassifier(max_depth=50, n_estimators=80, learning_rate=0.1, colsample_bytree=.7, gamma=0, reg_alpha=4, objective='binary:logistic', eta=0.3, silent=1, subsample=0.8).fit(X_train, y_train) \n",
        "xgb_prediction = xgb_model.predict(X_valid)\n",
        "\n",
        "from sklearn.metrics import f1_score, classification_report, accuracy_score\n",
        "\n",
        "print('training score:', f1_score(y_train, xgb_model.predict(X_train), average='macro'))\n",
        "print('validation score:', f1_score(y_valid, xgb_model.predict(X_valid), average='macro'))\n",
        "print(classification_report(y_valid, xgb_prediction))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training score: 0.8294809581438256\n",
            "validation score: 0.7586636708636403\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.90      0.84     84267\n",
            "           1       0.78      0.60      0.67     49148\n",
            "\n",
            "    accuracy                           0.79    133415\n",
            "   macro avg       0.78      0.75      0.76    133415\n",
            "weighted avg       0.79      0.79      0.78    133415\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKbrHlu5qYZn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "outputId": "a14f6506-21a0-4617-9ae8-67114d1e08a2"
      },
      "source": [
        "# word level implementation of tf-idf\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
        "tfidf_vect.fit(pd.concat((df['question1'],df['question2'])).unique())\n",
        "trainq1_trans = tfidf_vect.transform(df['question1'].values)\n",
        "trainq2_trans = tfidf_vect.transform(df['question2'].values)\n",
        "labels = df['is_duplicate'].values\n",
        "X = scipy.sparse.hstack((trainq1_trans,trainq2_trans))\n",
        "y = labels\n",
        "X_train,X_valid,y_train,y_valid = train_test_split(X,y, test_size = 0.33, random_state = 42)\n",
        "\n",
        "xgb_model = xgb.XGBClassifier(max_depth=50, n_estimators=80, learning_rate=0.1, colsample_bytree=.7, gamma=0, reg_alpha=4, objective='binary:logistic', eta=0.3, silent=1, subsample=0.8).fit(X_train, y_train) \n",
        "xgb_prediction = xgb_model.predict(X_valid)\n",
        "print('word level tf-idf training score:', f1_score(y_train, xgb_model.predict(X_train), average='macro'))\n",
        "print('word level tf-idf validation score:', f1_score(y_valid, xgb_model.predict(X_valid), average='macro'))\n",
        "print(classification_report(y_valid, xgb_prediction))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "word level tf-idf training score: 0.8890962443167643\n",
            "word level tf-idf validation score: 0.7616678142893527\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.90      0.84     84267\n",
            "           1       0.78      0.60      0.68     49148\n",
            "\n",
            "    accuracy                           0.79    133415\n",
            "   macro avg       0.79      0.75      0.76    133415\n",
            "weighted avg       0.79      0.79      0.78    133415\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyjbhC04Ff4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "outputId": "ef7cb274-4aa0-4e32-e3e2-91c493899a2c"
      },
      "source": [
        "# n-gram implementation of tf-idf\n",
        "\n",
        "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
        "tfidf_vect_ngram.fit(pd.concat((df['question1'],df['question2'])).unique())\n",
        "trainq1_trans = tfidf_vect_ngram.transform(df['question1'].values)\n",
        "trainq2_trans = tfidf_vect_ngram.transform(df['question2'].values)\n",
        "labels = df['is_duplicate'].values\n",
        "X = scipy.sparse.hstack((trainq1_trans,trainq2_trans))\n",
        "y = labels\n",
        "X_train,X_valid,y_train,y_valid = train_test_split(X,y, test_size = 0.33, random_state = 42)\n",
        "\n",
        "xgb_model = xgb.XGBClassifier(max_depth=50, n_estimators=80, learning_rate=0.1, colsample_bytree=.7, gamma=0, reg_alpha=4, objective='binary:logistic', eta=0.3, silent=1, subsample=0.8).fit(X_train, y_train) \n",
        "xgb_prediction = xgb_model.predict(X_valid)\n",
        "print('n-gram level tf-idf training score:', f1_score(y_train, xgb_model.predict(X_train), average='macro'))\n",
        "print('n-gram level tf-idf validation score:', f1_score(y_valid, xgb_model.predict(X_valid), average='macro'))\n",
        "print(classification_report(y_valid, xgb_prediction))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "n-gram level tf-idf training score: 0.7429326180629692\n",
            "n-gram level tf-idf validation score: 0.6785375801994364\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.91      0.81     84267\n",
            "           1       0.75      0.43      0.54     49148\n",
            "\n",
            "    accuracy                           0.74    133415\n",
            "   macro avg       0.74      0.67      0.68    133415\n",
            "weighted avg       0.74      0.74      0.71    133415\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVCuZ-zMFt62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "outputId": "bb105f91-58dc-4d07-da6f-46d9cb8e0425"
      },
      "source": [
        "# character level implementation of tf-idf\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', ngram_range=(2,3), max_features=5000)\n",
        "tfidf_vect_ngram_chars.fit(pd.concat((df['question1'],df['question2'])).unique())\n",
        "trainq1_trans = tfidf_vect_ngram_chars.transform(df['question1'].values)\n",
        "trainq2_trans = tfidf_vect_ngram_chars.transform(df['question2'].values)\n",
        "labels = df['is_duplicate'].values\n",
        "X = scipy.sparse.hstack((trainq1_trans,trainq2_trans))\n",
        "y = labels\n",
        "X_train,X_valid,y_train,y_valid = train_test_split(X,y, test_size = 0.33, random_state = 42)\n",
        "xgb_model = xgb.XGBClassifier(max_depth=50, n_estimators=80, learning_rate=0.1, colsample_bytree=.7, gamma=0, reg_alpha=4, objective='binary:logistic', eta=0.3, silent=1, subsample=0.8).fit(X_train, y_train) \n",
        "xgb_prediction = xgb_model.predict(X_valid)\n",
        "print('character level tf-idf training score:', f1_score(y_train, xgb_model.predict(X_train), average='macro'))\n",
        "print('character level tf-idf validation score:', f1_score(y_valid, xgb_model.predict(X_valid), average='macro'))\n",
        "print(classification_report(y_valid, xgb_prediction))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "character level tf-idf training score: 0.9977766871143036\n",
            "character level tf-idf validation score: 0.801555390342251\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.91      0.87     84267\n",
            "           1       0.82      0.67      0.74     49148\n",
            "\n",
            "    accuracy                           0.82    133415\n",
            "   macro avg       0.82      0.79      0.80    133415\n",
            "weighted avg       0.82      0.82      0.82    133415\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}